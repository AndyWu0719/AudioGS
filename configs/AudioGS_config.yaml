# AudioGS Configuration: VAE-based Encoder with VITS Loss Configuration
# =========================================================

# Data
data:
  dataset_path: "/data0/determined/users/andywu/GS-TS/data/raw/LibriTTS_R"
  sample_rate: 24000
  subsets: ["train-clean-100", "train-clean-360"]
  val_subsets: ["dev-clean"]
  max_audio_length: 2.0

# Encoder Model (VAE)
encoder_model:
  input_mel_bins: 80
  hidden_channels: 256
  grid_freq_bins: 128
  atoms_per_cell: 2
  time_downsample_factor: 240     # Mel hop: 24000/240 = 100Hz
  latent_downsample_ratio: 2      # Latent: 100Hz/2 = 50Hz
  num_layers: 6
  use_conformer: true

# Training (Step-Based)
training:
  batch_size: 8             # Small batch for VAE+GAN
  accumulation_steps: 4     # Gradient accumulation
  num_workers: 8
  learning_rate: 0.0002     # G learning rate
  max_steps: 100000
  warmup_steps: 1000
  val_interval: 2000
  save_interval: 10000
  log_interval: 10

# GAN Training
gan:
  enable: true
  d_lr: 0.0001            # Reduced from 2e-4 for stability
  fm_weight: 0.5          # Reduced from 2.0 to prevent early explosion
  adv_weight: 1.0           # VITS standard
  warmup_steps: 5000      # Do not train D for first 5k steps

# Loss (VITS Standard Configuration)
loss:
  # Reconstruction
  mel_weight: 45.0          # VITS standard (L1 Mel Loss)
  spectral_weight: 1.0      # Multi-resolution STFT
  time_domain_weight: 0.1
  phase_weight: 2.0
  pre_emp_weight: 10.0
  
  # VAE
  kl_weight: 1.0            # VITS standard
  
  # Regularization
  sparsity_weight: 0.001
  amp_reg_weight: 0.01

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false  # Better performance