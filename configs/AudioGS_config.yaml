# AudioGS Configuration: Amortized Inference (Encoder-Based) + GAN Training
# =========================================================

# Data
data:
  dataset_path: "/data0/determined/users/andywu/GS-TS/data/raw/LibriTTS_R"
  sample_rate: 24000
  subsets: ["train-clean-100", "train-clean-360"]
  val_subsets: ["dev-clean"]
  max_audio_length: 10.0

# Encoder Model
encoder_model:
  input_mel_bins: 80
  hidden_channels: 256
  grid_freq_bins: 128
  atoms_per_cell: 2
  time_downsample_factor: 240  # 24000Hz / 240 = 100Hz frame rate
  num_layers: 6
  use_conformer: true
  use_checkpointing: true  # Gradient checkpointing for memory savings

# Training (Step-Based with AMP)
training:
  batch_size: 4
  accumulation_steps: 8     # Effective batch = 4*8*4GPUs = 128
  learning_rate: 0.0002     # G learning rate (AdamW)
  max_steps: 100000
  warmup_steps: 1000
  warmup_freeze_structure: 2000
  val_interval: 2000
  save_interval: 10000
  log_interval: 10

# GAN Training
gan:
  enable: true
  d_lr: 0.0002              # Discriminator learning rate
  fm_weight: 10.0           # Feature matching loss weight
  adv_weight: 1.0           # Adversarial loss weight

# Loss (Rebalanced for GAN training)
loss:
  fft_sizes: [2048, 1024, 512, 128]
  hop_sizes: [512, 256, 128, 32]
  win_lengths: [2048, 1024, 512, 128]
  spectral_weight: 1.0
  mel_weight: 25.0          # High for GAN stability
  time_domain_weight: 0.1
  phase_weight: 2.0
  sparsity_weight: 0.001
  amp_reg_weight: 0.01
  pre_emp_weight: 10.0

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false