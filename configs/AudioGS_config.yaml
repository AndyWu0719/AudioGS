# AudioGS Configuration: Amortized Inference (Encoder-Based) + GAN Training
# =========================================================

# Data
data:
  dataset_path: "/data0/determined/users/andywu/GS-TS/data/raw/LibriTTS_R"
  sample_rate: 24000
  subsets: ["train-clean-100", "train-clean-360"]
  val_subsets: ["dev-clean"]
  max_audio_length: 3.0      # Reduced from 10.0 for memory efficiency

# Encoder Model
encoder_model:
  input_mel_bins: 80
  hidden_channels: 256
  grid_freq_bins: 128
  atoms_per_cell: 2
  time_downsample_factor: 240  # 24000Hz / 240 = 100Hz frame rate
  num_layers: 6
  use_conformer: true
  use_checkpointing: true  # Gradient checkpointing for memory savings

# Training (Step-Based)
training:
  # Phase 1 (no D): larger batch for better gradient estimation
  batch_size_phase1: 8
  # Phase 2 (with D): reduced batch due to D memory
  batch_size_phase2: 2
  accumulation_steps: 32       # Adjust to keep effective batch ~64
  learning_rate: 0.0002       # G learning rate (AdamW)
  max_steps: 100000
  warmup_steps: 1000
  warmup_freeze_structure: 5000
  val_interval: 2000
  save_interval: 10000
  log_interval: 10

# GAN Training
gan:
  enable: true
  d_lr: 0.0002              # Discriminator learning rate
  fm_weight: 10.0           # Feature matching loss weight
  adv_weight: 1.0           # Adversarial loss weight
  warmup_steps: 5000       # Extended Phase 1 for better pre-training

# Loss (Rebalanced for staged training)
loss:
  fft_sizes: [2048, 1024, 512, 128]
  hop_sizes: [512, 256, 128, 32]
  win_lengths: [2048, 1024, 512, 128]
  spectral_weight: 1.0
  mel_weight: 45.0          # High for strong reconstruction signal
  time_domain_weight: 0.1
  phase_weight: 2.0
  sparsity_weight: 0.1      # Increased for Target Ratio mechanism
  target_active_ratio: 0.25 # Target 25% active atoms (over-complete)
  amp_reg_weight: 0.01
  pre_emp_weight: 2.0       # Reduced to avoid over-penalizing high freq

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false