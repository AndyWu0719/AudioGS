# AudioGS Configuration: Amortized Inference (Encoder-Based) + GAN Training
# =========================================================

# Data
data:
  dataset_path: "/data0/determined/users/andywu/GS-TS/data/raw/LibriTTS_R"
  sample_rate: 24000
  subsets: ["train-clean-100", "train-clean-360"]
  val_subsets: ["dev-clean"]
  max_audio_length: 10.0

# Encoder Model
encoder_model:
  input_mel_bins: 80
  hidden_channels: 256
  grid_freq_bins: 128
  atoms_per_cell: 2
  time_downsample_factor: 240  # 24000Hz / 240 = 100Hz frame rate
  num_layers: 6
  use_conformer: true

# Training (Step-Based)
training:
  batch_size: 2            # Reduced for GAN training
  learning_rate: 0.0002     # G learning rate (AdamW)
  max_steps: 100000
  warmup_steps: 1000
  warmup_freeze_structure: 3000  # Freeze delta_tau/omega/sigma gradients for first 3k steps
  val_interval: 2000
  save_interval: 10000
  log_interval: 10

# GAN Training
gan:
  enable: true
  d_lr: 0.0002              # Discriminator learning rate
  fm_weight: 10.0           # Feature matching loss weight
  adv_weight: 1.0           # Adversarial loss weight
  d_steps: 1                # D updates per G update

# Loss (Rebalanced for GAN training)
# mel_weight increased to balance against fm_weight (HiFi-GAN style)
loss:
  fft_sizes: [2048, 1024, 512, 128]
  hop_sizes: [512, 256, 128, 32]
  win_lengths: [2048, 1024, 512, 128]
  spectral_weight: 1.0
  mel_weight: 25.0          # Increased from 0.5 for GAN stability
  time_domain_weight: 0.1
  phase_weight: 2.0         # Increased for phase alignment
  sparsity_weight: 0.001    # Energy-based sparsity
  amp_reg_weight: 0.01
  pre_emp_weight: 10.0

# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false